{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fc4438f-af6b-40d7-a5f8-729b3308ccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a5313df-3876-42a7-92dd-82265e31aa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85ae23fa-a104-4592-b53a-9330d83f138e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a615c9c-2a6d-4000-8e06-005fe75427da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c46983b5-af4a-4ffe-a2ef-3cab247f1c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok biolerplate done, now we get to the action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b1ed095-85a8-421d-a412-5244497a6646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1457c0b6-0e97-4838-a5ff-a417afc9a220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b76f95e-59d4-4ec2-9ae0-601f8dc5c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "918edf01-76e9-4a05-8c0d-6ae5ead212a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3031, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9b99661-359c-4d5a-9e96-856f167d7209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([27, 10]), torch.Size([32, 3, 10]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape, emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34aeb412-2246-463b-b3af-1e1adce1c746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "bngain          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "bnbias          | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
      "bnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "bnvar_inv       | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "bnvar           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "bndiff2         | exact: False | approximate: True  | maxdiff: 4.3655745685100555e-11\n",
      "bndiff          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "bnmeani         | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "embcat          | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
      "W1              | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\n",
      "b1              | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\n",
      "emb             | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
      "C               | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "dprobs = (1.0/probs) * dlogprobs\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "dcounts_sum = (-1 * counts_sum ** -2) * dcounts_sum_inv\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "dnorm_logits = counts * dcounts\n",
    "dlogit_maxes = -dnorm_logits.sum(1, keepdim=True)\n",
    "dlogits = dnorm_logits.clone()\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "dbnraw = (bngain * dhpreact)\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "dbnvar = -0.5 * (bnvar + 1e-05)**-1.5 * dbnvar_inv\n",
    "dbndiff2 = dbnvar / (n - 1)\n",
    "dbndiff = 2 * bndiff * dbndiff2 + dbnraw * bnvar_inv\n",
    "dbnmeani = (-torch.ones_like(bnmeani) * dbndiff).sum(0, keepdim=True)\n",
    "dhprebn = torch.ones_like(hprebn) * dbndiff\n",
    "dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0, keepdim=True)\n",
    "demb = dembcat.view(emb.shape)\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k,j]\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "187bd90f-60b1-4ac6-a81e-e08529b77d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 5.820766091346741e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "dlogits = F.softmax(logits, 1)\n",
    "dlogits[range(n), Yb] -= 1\n",
    "dlogits /= n\n",
    "\n",
    "cmp('logits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7298df71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7ab75bb470>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "076cd16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dhprebn         | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "\n",
    "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "\n",
    "cmp('dhprebn', dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94ddb2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.7852\n",
      "  10000/ 200000: 2.1844\n",
      "  20000/ 200000: 2.3863\n",
      "  30000/ 200000: 2.4293\n",
      "  40000/ 200000: 1.9761\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "\n",
    "  # kick off optimization\n",
    "  for i in range(max_steps):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    # Linear layer\n",
    "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "    # BatchNorm layer\n",
    "    # -------------------------------------------------------------\n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    # -------------------------------------------------------------\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "      p.grad = None\n",
    "    #loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "    # manual backprop! #swole_doge_meme\n",
    "    # -----------------\n",
    "    dlogits = F.softmax(logits, 1)\n",
    "    dlogits[range(n), Yb] -= 1\n",
    "    dlogits /= n\n",
    "    # 2nd Layer backprop\n",
    "    dh = dlogits @ W2.T\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(0)\n",
    "    #tanh\n",
    "    dhpreact = (1.0 - h**2) * dh\n",
    "    #batchnorm backprop\n",
    "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "    # 1st Layer\n",
    "    dembcat = dhprebn @ W1.T\n",
    "    dW1 = embcat.T @ dhprebn\n",
    "    db1 = dhprebn.sum(0)\n",
    "    # Embedding\n",
    "    demb = dembcat.view(emb.shape)\n",
    "    dC = torch.zeros_like(C)\n",
    "    for k in range(Xb.shape[0]):\n",
    "        for j in range(Xb.shape[1]):\n",
    "            ix = Xb[k,j]\n",
    "            dC[ix] += demb[k,j]\n",
    "\n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "    # -----------------\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "    for p, grad in zip(parameters, grads):\n",
    "      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "      p.data += -lr * grad #b new way of swole doge TODO: enable\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "  #   if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "  #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2dd6eb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
